"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –∏ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ü–µ–ø—è—Ö –ú–∞—Ä–∫–æ–≤–∞
"""
import streamlit as st
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from .markov_chain import CryptoMarkovChain

class TimeframeAnalysis:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ü–µ–ø–µ–π –ú–∞—Ä–∫–æ–≤–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö"""
    
    def __init__(self):
        self.timeframes = {
            '1m': '1 –º–∏–Ω—É—Ç–∞',
            '5m': '5 –º–∏–Ω—É—Ç', 
            '15m': '15 –º–∏–Ω—É—Ç',
            '1h': '1 —á–∞—Å',
            '4h': '4 —á–∞—Å–∞',
            '1d': '1 –¥–µ–Ω—å',
            '1w': '1 –Ω–µ–¥–µ–ª—è'
        }
        self.markov_chains = {}
        self.fractal_analysis = {}
        
    def analyze_multiple_timeframes(self, data: Dict[str, pd.DataFrame]) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑ —Ü–µ–ø–µ–π –ú–∞—Ä–∫–æ–≤–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º {timeframe: DataFrame}
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –≤—Å–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        """
        results = {}
        
        for timeframe, df in data.items():
            st.write(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º: {self.timeframes.get(timeframe, timeframe)}")
            
            # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø—å –ú–∞—Ä–∫–æ–≤–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            markov_chain = CryptoMarkovChain()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
            states = markov_chain.define_states(df)
            
            # –°—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É –ø–µ—Ä–µ—Ö–æ–¥–æ–≤
            transition_matrix = markov_chain.build_transition_matrix(df)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å
            stationary_analysis = self._analyze_stationarity(transition_matrix)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç—å
            fractal_analysis = self._analyze_fractality(df, timeframe)
            
            results[timeframe] = {
                'markov_chain': markov_chain,
                'states': states,
                'transition_matrix': transition_matrix,
                'stationary_analysis': stationary_analysis,
                'fractal_analysis': fractal_analysis,
                'data_points': len(df),
                'unique_states': len(set(states.values()))
            }
            
            self.markov_chains[timeframe] = markov_chain
        
        return results
    
    def _analyze_stationarity(self, transition_matrix: np.ndarray) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã –ø–µ—Ä–µ—Ö–æ–¥–æ–≤"""
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        eigenvalues, eigenvectors = np.linalg.eig(transition_matrix.T)
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        stationary_idx = np.argmin(np.abs(eigenvalues - 1.0))
        stationary_vector = np.real(eigenvectors[:, stationary_idx])
        stationary_vector = np.abs(stationary_vector)
        stationary_vector = stationary_vector / np.sum(stationary_vector)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å
        is_stationary = np.allclose(transition_matrix.T @ stationary_vector, stationary_vector, atol=1e-10)
        
        # –≠–Ω—Ç—Ä–æ–ø–∏—è
        entropy = -np.sum(stationary_vector * np.log(stationary_vector + 1e-10))
        
        return {
            'is_stationary': is_stationary,
            'stationary_distribution': stationary_vector,
            'entropy': entropy,
            'max_probability': np.max(stationary_vector),
            'min_probability': np.min(stationary_vector)
        }
    
    def _analyze_fractality(self, data: pd.DataFrame, timeframe: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö
        volatility_scales = self._calculate_volatility_scales(data)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏
        correlation_analysis = self._analyze_cross_timeframe_correlations(data, timeframe)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∞–º–æ–ø–æ–¥–æ–±–∏–µ
        self_similarity = self._analyze_self_similarity(data)
        
        return {
            'volatility_scales': volatility_scales,
            'correlation_analysis': correlation_analysis,
            'self_similarity': self_similarity,
            'hurst_exponent': self._calculate_hurst_exponent(data['Close'])
        }
    
    def _calculate_volatility_scales(self, data: pd.DataFrame) -> Dict:
        """–†–∞—Å—á–µ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö"""
        scales = [1, 5, 10, 20, 50]  # –†–∞–∑–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞
        volatility_by_scale = {}
        
        for scale in scales:
            if len(data) >= scale:
                returns = data['Close'].pct_change(scale).dropna()
                volatility = returns.std() * np.sqrt(scale)  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
                volatility_by_scale[f'scale_{scale}'] = volatility
        
        return volatility_by_scale
    
    def _analyze_cross_timeframe_correlations(self, data: pd.DataFrame, timeframe: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏"""
        # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º
        correlations = {
            'intraday_patterns': self._analyze_intraday_patterns(data),
            'weekly_patterns': self._analyze_weekly_patterns(data),
            'monthly_patterns': self._analyze_monthly_patterns(data)
        }
        
        return correlations
    
    def _analyze_intraday_patterns(self, data: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–∏–¥–Ω–µ–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if 'hour' not in data.index.names and hasattr(data.index, 'hour'):
            data_with_hour = data.copy()
            data_with_hour['hour'] = data_with_hour.index.hour
        else:
            return {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —á–∞—Å–∞—Ö'}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —á–∞—Å–∞–º
        hourly_volatility = data_with_hour.groupby('hour')['Close'].std()
        hourly_returns = data_with_hour.groupby('hour')['Close'].mean()
        
        return {
            'hourly_volatility': hourly_volatility.to_dict(),
            'hourly_returns': hourly_returns.to_dict(),
            'peak_hour': hourly_volatility.idxmax(),
            'quiet_hour': hourly_volatility.idxmin()
        }
    
    def _analyze_weekly_patterns(self, data: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ–¥–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if hasattr(data.index, 'dayofweek'):
            data_with_dow = data.copy()
            data_with_dow['dayofweek'] = data_with_dow.index.dayofweek
        else:
            return {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –¥–Ω—è—Ö –Ω–µ–¥–µ–ª–∏'}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        daily_volatility = data_with_dow.groupby('dayofweek')['Close'].std()
        daily_returns = data_with_dow.groupby('dayofweek')['Close'].mean()
        
        day_names = ['–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–í—Ç–æ—Ä–Ω–∏–∫', '–°—Ä–µ–¥–∞', '–ß–µ—Ç–≤–µ—Ä–≥', '–ü—è—Ç–Ω–∏—Ü–∞', '–°—É–±–±–æ—Ç–∞', '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ']
        
        return {
            'daily_volatility': {day_names[i]: daily_volatility.get(i, 0) for i in range(7)},
            'daily_returns': {day_names[i]: daily_returns.get(i, 0) for i in range(7)},
            'most_volatile_day': day_names[daily_volatility.idxmax()],
            'least_volatile_day': day_names[daily_volatility.idxmin()]
        }
    
    def _analyze_monthly_patterns(self, data: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Å—è—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if hasattr(data.index, 'month'):
            data_with_month = data.copy()
            data_with_month['month'] = data_with_month.index.month
        else:
            return {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –º–µ—Å—è—Ü–∞—Ö'}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º
        monthly_volatility = data_with_month.groupby('month')['Close'].std()
        monthly_returns = data_with_month.groupby('month')['Close'].mean()
        
        month_names = ['–Ø–Ω–≤', '–§–µ–≤', '–ú–∞—Ä', '–ê–ø—Ä', '–ú–∞–π', '–ò—é–Ω', 
                      '–ò—é–ª', '–ê–≤–≥', '–°–µ–Ω', '–û–∫—Ç', '–ù–æ—è', '–î–µ–∫']
        
        return {
            'monthly_volatility': {month_names[i-1]: monthly_volatility.get(i, 0) for i in range(1, 13)},
            'monthly_returns': {month_names[i-1]: monthly_returns.get(i, 0) for i in range(1, 13)},
            'most_volatile_month': month_names[monthly_volatility.idxmax() - 1],
            'least_volatile_month': month_names[monthly_volatility.idxmin() - 1]
        }
    
    def _analyze_self_similarity(self, data: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–∞–º–æ–ø–æ–¥–æ–±–∏—è (—Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç–∏)"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∞–º–æ–ø–æ–¥–æ–±–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö –≤—Ä–µ–º–µ–Ω–∏
        scales = [2, 4, 8, 16, 32]
        similarity_scores = {}
        
        for scale in scales:
            if len(data) >= scale * 2:
                # –ë–µ—Ä–µ–º –∫–∞–∂–¥—É—é scale-—é —Ç–æ—á–∫—É
                sampled_data = data.iloc[::scale]
                if len(sampled_data) >= 10:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    original_vol = data['Close'].pct_change().std()
                    sampled_vol = sampled_data['Close'].pct_change().std()
                    
                    similarity = 1 - abs(original_vol - sampled_vol) / original_vol
                    similarity_scores[f'scale_{scale}'] = max(0, similarity)
        
        return {
            'similarity_scores': similarity_scores,
            'average_similarity': np.mean(list(similarity_scores.values())) if similarity_scores else 0,
            'is_fractal': np.mean(list(similarity_scores.values())) > 0.7 if similarity_scores else False
        }
    
    def _calculate_hurst_exponent(self, prices: pd.Series) -> float:
        """–†–∞—Å—á–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –•–µ—Ä—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç–∏"""
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –•–µ—Ä—Å—Ç–∞
            returns = prices.pct_change().dropna()
            
            # –†–∞–∑–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫–Ω–∞
            windows = [2, 4, 8, 16, 32]
            if len(returns) < max(windows):
                return 0.5  # –°–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
            
            ranges = []
            scales = []
            
            for window in windows:
                if len(returns) >= window:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –æ–∫–Ω–∞
                    n_windows = len(returns) // window
                    if n_windows > 0:
                        windowed_returns = returns[:n_windows * window].values.reshape(n_windows, window)
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º R/S –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞
                        rs_values = []
                        for w in windowed_returns:
                            if len(w) > 1:
                                mean_w = np.mean(w)
                                deviations = w - mean_w
                                cumulative = np.cumsum(deviations)
                                R = np.max(cumulative) - np.min(cumulative)
                                S = np.std(w)
                                if S > 0:
                                    rs_values.append(R / S)
                        
                        if rs_values:
                            ranges.append(np.mean(rs_values))
                            scales.append(window)
            
            if len(ranges) >= 2:
                # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è log(R/S) vs log(scale)
                log_scales = np.log(scales)
                log_ranges = np.log(ranges)
                
                # –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
                n = len(log_scales)
                if n > 1:
                    slope = (n * np.sum(log_scales * log_ranges) - np.sum(log_scales) * np.sum(log_ranges)) / \
                           (n * np.sum(log_scales**2) - np.sum(log_scales)**2)
                    return slope
                else:
                    return 0.5
            else:
                return 0.5
                
        except Exception:
            return 0.5  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
    
    def plot_timeframe_comparison(self, results: Dict):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤"""
        timeframes = list(results.keys())
        
        # –ì—Ä–∞—Ñ–∏–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        fig1 = go.Figure()
        
        states_count = [results[tf]['unique_states'] for tf in timeframes]
        data_points = [results[tf]['data_points'] for tf in timeframes]
        
        fig1.add_trace(go.Bar(
            x=[self.timeframes.get(tf, tf) for tf in timeframes],
            y=states_count,
            name='–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è',
            marker_color='blue'
        ))
        
        fig1.update_layout(
            title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º",
            xaxis_title="–¢–∞–π–º—Ñ—Ä–µ–π–º",
            yaxis_title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π",
            height=400
        )
        
        st.plotly_chart(fig1, use_container_width=True)
        
        # –ì—Ä–∞—Ñ–∏–∫ —ç–Ω—Ç—Ä–æ–ø–∏–∏
        fig2 = go.Figure()
        
        entropies = [results[tf]['stationary_analysis']['entropy'] for tf in timeframes]
        
        fig2.add_trace(go.Scatter(
            x=[self.timeframes.get(tf, tf) for tf in timeframes],
            y=entropies,
            mode='lines+markers',
            name='–≠–Ω—Ç—Ä–æ–ø–∏—è',
            line=dict(color='red', width=3),
            marker=dict(size=8)
        ))
        
        fig2.update_layout(
            title="–≠–Ω—Ç—Ä–æ–ø–∏—è –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º",
            xaxis_title="–¢–∞–π–º—Ñ—Ä–µ–π–º",
            yaxis_title="–≠–Ω—Ç—Ä–æ–ø–∏—è",
            height=400
        )
        
        st.plotly_chart(fig2, use_container_width=True)
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç–∏
        fig3 = go.Figure()
        
        hurst_exponents = [results[tf]['fractal_analysis']['hurst_exponent'] for tf in timeframes]
        
        fig3.add_trace(go.Scatter(
            x=[self.timeframes.get(tf, tf) for tf in timeframes],
            y=hurst_exponents,
            mode='lines+markers',
            name='–≠–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ –•–µ—Ä—Å—Ç–∞',
            line=dict(color='green', width=3),
            marker=dict(size=8)
        ))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        fig3.add_hline(y=0.5, line_dash="dash", line_color="gray", 
                      annotation_text="–°–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ (0.5)")
        fig3.add_hline(y=0.7, line_dash="dash", line_color="orange", 
                      annotation_text="–¢—Ä–µ–Ω–¥ (0.7)")
        fig3.add_hline(y=0.3, line_dash="dash", line_color="purple", 
                      annotation_text="–°—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ (0.3)")
        
        fig3.update_layout(
            title="–§—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º (–≠–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ –•–µ—Ä—Å—Ç–∞)",
            xaxis_title="–¢–∞–π–º—Ñ—Ä–µ–π–º",
            yaxis_title="–≠–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ –•–µ—Ä—Å—Ç–∞",
            height=400
        )
        
        st.plotly_chart(fig3, use_container_width=True)
    
    def generate_trading_insights(self, results: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤"""
        insights = {
            'best_timeframes': [],
            'fractal_insights': [],
            'volatility_insights': [],
            'recommendations': []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª—É—á—à–∏–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
        for tf, result in results.items():
            entropy = result['stationary_analysis']['entropy']
            hurst = result['fractal_analysis']['hurst_exponent']
            states_count = result['unique_states']
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è "—Ö–æ—Ä–æ—à–µ–≥–æ" —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            score = 0
            if entropy > 2.0:  # –í—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è = –º–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                score += 1
            if 0.4 < hurst < 0.6:  # –ë–ª–∏–∑–∫–æ –∫ —Å–ª—É—á–∞–π–Ω–æ–º—É –±–ª—É–∂–¥–∞–Ω–∏—é = –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ
                score += 1
            if states_count > 10:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                score += 1
            
            if score >= 2:
                insights['best_timeframes'].append({
                    'timeframe': tf,
                    'score': score,
                    'entropy': entropy,
                    'hurst': hurst
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
        insights['best_timeframes'].sort(key=lambda x: x['score'], reverse=True)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ—Å—Ç—å
        fractal_tfs = [tf for tf, result in results.items() 
                      if result['fractal_analysis']['is_fractal']]
        
        if fractal_tfs:
            insights['fractal_insights'].append(
                f"–§—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–∞ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö: {', '.join(fractal_tfs)}"
            )
            insights['recommendations'].append(
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö"
            )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        volatility_analysis = {}
        for tf, result in results.items():
            vol_scales = result['fractal_analysis']['volatility_scales']
            if vol_scales:
                avg_vol = np.mean(list(vol_scales.values()))
                volatility_analysis[tf] = avg_vol
        
        if volatility_analysis:
            most_volatile = max(volatility_analysis, key=volatility_analysis.get)
            least_volatile = min(volatility_analysis, key=volatility_analysis.get)
            
            insights['volatility_insights'].extend([
                f"–°–∞–º—ã–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º: {most_volatile}",
                f"–°–∞–º—ã–π —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º: {least_volatile}"
            ])
        
        return insights
